"""
Created on Fri June 25 13:05:12 2021

@author: Mariano Barella

This script reads and processes HDF5 files generated by the Picasso software.
It extracts relevant data and saves it in various formats.

Refactored for improved maintainability and reduced code duplication.
"""

# ================ IMPORT LIBRARIES ================
import pickle
import numpy as np
import h5py
import os
import re
import tkinter as tk
import tkinter.filedialog as fd
from auxiliary_functions import manage_save_directory
from constants import get_default_config, STEP1_FILTER_SIGMA, STEP1_FILTER_UNCERTAINTY, STEP1_FILTER_PHOTONS

def save_filtered_data_to_files(data_dict, save_folder, clean_filename, filter_index):
    """Save filtered data arrays to individual files."""
    
    data_mappings = [
        ('frame', '_frame.dat', '%i'),
        ('positions', '_xy.dat', '%.3f'),
        ('std', '_sxy.dat', '%.3f'),
        ('lp', '_lpxy.dat', '%.3f'),
        ('ellipticity', '_ellipticity.dat', '%.1f'),
        ('netgrad', '_netgrad.dat', '%.1f'),
        ('photons', '_photons.dat', '%.1f'),
        ('bkg', '_bkg.dat', '%.1f'),
        ('pick_number', '_pick_number.dat', '%i')
    ]
    
    link_files_dict = {}
    
    for key, suffix, fmt in data_mappings:
        if key in data_dict:
            data_to_save = data_dict[key]
            new_filename = clean_filename + suffix
            new_filepath = os.path.join(save_folder, new_filename)
            np.savetxt(new_filepath, data_to_save, fmt=fmt)
            link_files_dict[key] = new_filename
    
    return link_files_dict


def extract_and_filter_hdf5_data(filepath, rectangles_flag, lpx_filter, lpy_filter):
    """Extract and filter data from HDF5 file."""
    
    with h5py.File(filepath, 'r') as f:
        a_group_key = list(f.keys())[0]
        data = list(f[a_group_key])

    arrays = {}
    array_names = ['frame', 'x', 'y', 'photons', 'sx', 'sy', 'bg', 'lpx', 'lpy', 
                   'ellipticity', 'net_gradient', 'group']
    
    for name in array_names:
        arrays[name] = np.zeros(len(data))

    for i in range(len(data)):
        arrays['frame'][i] = data[i][0]
        arrays['x'][i] = data[i][1]
        arrays['y'][i] = data[i][2]
        arrays['photons'][i] = data[i][3]
        arrays['sx'][i] = data[i][4]
        arrays['sy'][i] = data[i][5]
        arrays['bg'][i] = data[i][6]
        arrays['lpx'][i] = data[i][7]
        arrays['lpy'][i] = data[i][8]
        arrays['ellipticity'][i] = data[i][9]
        arrays['net_gradient'][i] = data[i][10]
        
        if rectangles_flag:
            arrays['group'][i] = data[i][13]
        else:
            arrays['group'][i] = data[i][11]

    lpx_filter_low, lpx_filter_high = (0.005, lpx_filter)
    lpy_filter_low, lpy_filter_high = (0.005, lpy_filter)
    
    filter_index = np.where(
        (arrays['lpx'] > lpx_filter_low) & 
        (arrays['lpy'] > lpy_filter_low) & 
        (arrays['lpx'] < lpx_filter_high) & 
        (arrays['lpy'] < lpy_filter_high) & 
        (arrays['net_gradient'] > 800)
    )
    
    filtered_data = {
        'frame': arrays['frame'][filter_index],
        'positions': np.asarray([arrays['x'][filter_index], arrays['y'][filter_index]]).T,
        'std': np.asarray([arrays['sx'][filter_index], arrays['sy'][filter_index]]).T,
        'lp': np.asarray([arrays['lpx'][filter_index], arrays['lpy'][filter_index]]).T,
        'ellipticity': arrays['ellipticity'][filter_index],
        'netgrad': arrays['net_gradient'][filter_index],
        'photons': arrays['photons'][filter_index],
        'bkg': arrays['bg'][filter_index],
        'pick_number': arrays['group'][filter_index]
    }
    
    return filtered_data, filter_index, arrays


def split_hdf5(hdf5_file, folder, recursive_flag, rectangles_flag, lpx_filter,
               lpy_filter, verbose_flag, NP_flag=False):
    """
    Main function to process HDF5 files and extract localization data.
    
    Args:
        hdf5_file: Path to the HDF5 file
        folder: Base folder path
        recursive_flag: Process all HDF5 files in folder
        rectangles_flag: Whether picks are rectangles
        lpx_filter: LPX filter threshold
        lpy_filter: LPY filter threshold
        verbose_flag: Enable verbose output
        NP_flag: Whether to process NP data
    """
    
    print('\nStarting STEP 1.')
    folder = os.path.dirname(hdf5_file)
    video_name = os.path.basename(hdf5_file)
    
    if recursive_flag:
        list_of_files = [f for f in os.listdir(folder) if re.search('.hdf5', f)]
        list_of_files.sort()
    elif NP_flag:
        list_of_files = [f for f in os.listdir(folder) 
                        if re.search('NP_subtracted', f) and re.search('picked.hdf5', f)]
        if len(list_of_files) == 2:
            list_of_files = [list_of_files[0]]
        list_of_files.append(video_name)
    else:
        list_of_files = [video_name]
    
    save_folder = os.path.join(folder, 'analysis', 'step1', 'data')
    if not os.path.exists(save_folder):
        os.makedirs(save_folder)
    
    total_localizations_before = 0
    total_localizations_after = 0
    all_pick_numbers = []
    
    for filename in list_of_files:
        filepath = os.path.join(folder, filename)
        print('\nFile selected:', filepath)
        
        filtered_data, filter_index, original_arrays = extract_and_filter_hdf5_data(
            filepath, rectangles_flag, lpx_filter, lpy_filter
        )
        
        clean_filename = filename[:-5].replace('MMStack_Pos0.ome_', '')
        
        if NP_flag:
            clean_filename = 'NP_subtracted' if 'NP' in filename else 'raw'
        else:
            clean_filename = ''
        
        link_files_dict = save_filtered_data_to_files(
            filtered_data, save_folder, clean_filename, filter_index
        )
        
        dict_filename = clean_filename + '_dict.pkl'
        dict_filepath = os.path.join(save_folder, dict_filename)
        with open(dict_filepath, 'wb') as f:
            pickle.dump(link_files_dict, f)
        
        total_localizations_before += len(original_arrays['frame'])
        total_localizations_after += len(filtered_data['frame'])
        all_pick_numbers.extend(np.unique(filtered_data['pick_number']))
    
    unique_picks = len(np.unique(all_pick_numbers))
    
    print('\n' + '='*23 + 'ðŸ”¬ STEP 1 SUMMARY ðŸ”¬' + '='*23)
    print(f'   Total Files Processed: {len(list_of_files)}')
    print(f'   Picks Found: {unique_picks}')
    print(f'   Localizations Before LP Filter: {total_localizations_before}')
    print(f'   Localizations After LP Filter: {total_localizations_after}')
    print(f'   All data saved in "analysis/step1/data" directory.')
    print('='*70)
    print('\nDone with STEP 1.')
    
    return {
        'total_files_processed': len(list_of_files),
        'picks_found': unique_picks,
        'localizations_before_filter': total_localizations_before,
        'localizations_after_filter': total_localizations_after,
        'filter_efficiency': total_localizations_after / total_localizations_before if total_localizations_before > 0 else 0
    }

#####################################################################
#####################################################################
#####################################################################

if __name__ == '__main__':
    
    config = get_default_config()
    
    rectangles_flag = False
    recursive_flag = False
    NP_flag = False
    verbose_flag = True
    
    lpx_filter = 0.15
    lpy_filter = 0.15
    
    # Load and open folder and file
    root = tk.Tk()
    hdf5_file = fd.askopenfilename(
        initialdir=config['base_folder'],
        filetypes=(("HDF5 files", "*.hdf5"), ("All files", "*.*"))
    )
    root.withdraw()
    
    if hdf5_file:
        results = split_hdf5(hdf5_file, config['base_folder'], recursive_flag, 
                           rectangles_flag, lpx_filter, lpy_filter, verbose_flag, NP_flag)
    else:
        print("No file selected. Exiting.")
